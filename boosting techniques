Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers,
building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added,
are the special algorithms that are used to augment the existing result of the data model and help to fix the errors
There are three types: AdaBoost or Adaptive boosting Algorithm, Gradient, and XG Boosting algorithm

AdaBoost is best used to boost the performance of decision trees on binary classification problems,
The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps
It combines the group of weak learner base on weight age to create a strong learner
 In the first iteration, it gives equal weight to each data set and the starts predicting that data set. If incorrect prediction occurs, it gives high weight to that observation.
 
 
Gradient boosting algorithm is a machine learning technique to define loss function and reduce it. It is used to solve problems of classification using prediction models
it invloves: loss function, weak learner, additive model


XG Boost is short for Extreme Gradient Boosting. XG Boost is an upgraded implementation of the Gradient Boosting Algorithm, which is developed for high computational speed, scalability, and better performance.
it's features: parallel processing, cross validation, cache optimization, distributed computing
